{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 线性回归 \n",
    ">回归和相关有着紧密的联系,在复习回归之前,我们先复习相关性的概念.\n",
    "\n",
    ">在统计学基本知识的复习里,我们复习的统计量和参数都是一次描述一个变量,尽管单个变量的统计很重要,但很多时候我们会对两个及以上变量之间的关系更感兴趣：学生的考试成绩和他的备考时间有关系吗？日平均气温和冷饮的销量有关系吗？等等诸如此类的问题需要引入变量之间的关联程度的统计量：相关系数.\n",
    "\n",
    "## 相关系数\n",
    ">现实世界中,不同的变量之间经常表现出某种相关关系.例如,一般来说,身高较高的人,体重也相对较重,等等.然而这种相关关系很多时候并不能一眼就看出来,这就需要我们使用统计学工具来根据已有的样本数据去考察两个变量之间是否真的有某种相关性.\n",
    ">根据两个变量的类型等不同等因素,相关系数也有多种类型.以下我们以皮尔逊积差相关系数为例进行复习.\n",
    "### 皮尔逊积差相关系数\n",
    ">皮尔逊积差相关系数考察的是两个定比或定距变量之间的相关性.\n",
    ">我们关心两个变量之间的相关性,实际上主要关心的是两个方面的因素：\n",
    "* 相关性的方向.  \n",
    "   正相关意味着我们分析的两个变量的取值平均而言会同时增大或减小.而负相关则意味着两个变量的取值反方向变化：平均而言,一个变量增大时,另一个变量会减小.   \n",
    "   注意上述陈述中的“平均而言”四个字表明,可能存在和总的趋势相悖的例外值.\n",
    "* 相关性的强度或量级.  \n",
    "   相关性的强度的取值范围为[-1,1],特别的,当两个变量的相关系数为0时,表示两个变量没有直线相关关系,这时候在两个线性代数里可以看作样本数据在两个变量上的取值所组成的两个向量是线性无关的.  \n",
    "   一般而言,实际的社会科学研究中,完全正（负）相关都是极其罕见的.一些前人研究的经验法则告诉我们,相关系数绝对值小于0.2时,可以视作两个变量弱相关,相关系数绝对值位于0.2$\\sim$0.5时,称两个变量中等程度的相关,当相关系数的绝对值大于0.5时,表示强相关.\n",
    "\n",
    "\n",
    ">皮尔逊积差相关系数$r$的计算公式：\n",
    "$$r=\\frac{\\Sigma (z_x z_y)}{N}$$\n",
    "其中$z_x,z_y$分别为变量$X,Y$的$z$分数,$N$表示$X,Y$的配对个数,如果是在考察一个样本中对象的两个变量之间的关系,则这个数值就是样本所含的对象数.我们注意到上式实际上是两个$z$分数的交叉乘积的平均值,它相当于把两个变量的协方差进行了标准化.\n",
    "\n",
    "\n",
    ">**注意**:    \n",
    "相关性仅仅意味着一个变量取值的变动\\textbf{对应于}另一个变量取值的变动,除此之外,没有告诉我们任何其他的事情.绝不能从相关系数的计算得出两个变量之间的因果关系.作为对比,我们需要明确一下：因果关系意味着一个变量取值的变动\\textbf{导致了}另一个变量取值的变动.因果关系通常是借助于领域知识做出的决定,而不应该是相关性的计算.   \n",
    "关于相关和因果的关系,可以参阅《别拿相关当因果》这本有趣的书.    \n",
    "此外,我们还需注意,相关性仅仅考察了两个变量之间是否具有线性相关的关系,但很多时候两个变量之间的关系并不仅仅是简单的线性相关,比如,有很多变量之间的关系是曲线而不是直线.这个问题留待以后进一步讨论.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一元线性回归\n",
    ">当我们在考察两个变量之间的相关系数的时候,我们并不明确区分谁是因变量谁是自变量.这么做的不便之处在于,例如,我们想要通过一个变量的改变程度对另一个变量的改变进行预测,光有相关系数就不够用了,这时候就需要使用线性回归来确定自变量和因变量之间的函数关系,以便进行预测.回归和相关的区别是：相关分析不区分自变量和因变量,但回归一定区分自变量和因变量.\n",
    "\n",
    ">线性回归根据所涉及的变量个数而分为一元线性回归（或者称为简单回归）和多元线性回归.一元线性回归涉及一个自变量（或者称为预测变量）和一个因变量（也称为结果变量）,我们可以根据预测变量的给定值来对结果变量的取值进行预测.而多元回归则涉及多个自变量和一个因变量,多元回归分析使得我们能够考察多个自变量和因变量之间的关系的性质与强度、若干自变量对因变量的相对预测能力,以及在控制了一个或多个协变量的情况下,一个或多个自变量的独特贡献,此外还能检验交互效应.\n",
    "\n",
    ">我们这里讨论的回归所使用的自变量与因变量都必须是定距或定比变量.对于二分变量作为预测变量的回归,可以参考Logit回归（也称为Logistic regression,“逻辑回归”）.\n",
    "\n",
    "## 一元线性回归方程\n",
    ">一元线性回归的方程为\n",
    "$$\\hat{Y}=bX+a$$\n",
    "其中,$\\hat{Y}$为变量$Y$的预测值,$b$为未标准化的回归系数（或斜率）,$a$为截距,$X$为自变量.这里重点说明一下$b$和相关系数的关系.我们说“$b$为未标准化的回归系数”,其实就是说它经过标准化后,就得到了皮尔逊相关系数$r$.以下不加证明地给出二者之间的关系：\n",
    "$$b=r \\frac{s_Y}{s_X}$$\n",
    "上式中,$s_Y,s_X$分别为因变量$Y$和自变量$X$的标准差.\n",
    "\n",
    ">我们可以从量纲角度来考察上式.注意到相关系数$r$是一个比率,它是无量纲的,两个变量标准差的量纲分别和变量的量纲一直,标准差之比实现了将量纲从自变量转换为了因变量.因此在一元线性回归方程中,$b$实现了量纲从自变量到因变量的转换（当然事实上它还实现了将自变量的变化幅度转化为因变量的变化幅度）.\n",
    "\n",
    ">需要注意的是,上述一元线性回归方程只是预测了因变量的变化,而不是因变量的实际值,如果想要把上式方程中的预测值替换为实际值,需要增加一个误差项$e$,因此我们可以给出如下的另外一个线性回归方程\n",
    "$$Y=bX+a+e$$\n",
    "这个方程将误差项考虑在内,使得我们能够把实际的因变量观测值和自变量的值联系起来.\n",
    "\n",
    "## 最小二乘法\n",
    ">如果我们把自变量和因变量的值组成的有序偶视为直角坐标系中的点的坐标,则我们可以在坐标系中绘出 \\textbf{散点图},然后我们求回归方程的目的就变成了寻求穿过这些数据点的“最好”的直线.这里的“最好”,意味着尽管这条直线不一定穿过了最多的点,但是当我们计算所有点到这条之间的距离之和（注意这个和其实主体部分也是某种形式的平方和）的时候,我们希望这条“最好”的直线是所有直线中使得这个距离之和最小的直线.这就是最小二乘法的思想."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 卡方分布\n",
    ">卡方分布($\\chi ^2$分布)是从正态分布派生出来的一个分布.尽管如此,但它在数理统计中一直占据着重要的地位.此外,许多分布都可以用卡方分布来近似.甚至在多元统计中也常遇到卡方分布.\n",
    "\n",
    ">**定义** \n",
    "设$Z_1,Z_2,...,Z_n$独立且均服从标准正态分布,则随机变量$X=\\sum^{n}_{i=1}Z^{2}_i$的分布称为具有$n$个自由度的$\\chi ^2$分布"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 方差分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">一个变量的方差有多少可以与另一个变量共享,或由另一个变量所解释？这个问题就是方差分析的核心问题.我们在假设检验章节中复习的$F$检验方法时提到,方差分析是$F$检验的一种重要形式,它是通过计算$F$值来进行的假设检验.\n",
    ">方差分析的前提条件是：\n",
    "* 所有样本都是相互独立的；\n",
    "* 所有样本来源的总体都服从正态分布；\n",
    "* 所有总体的方差都相等（即所谓的\\textbf{方差齐性}）,但方差的值是未知的；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 单因子方差分析\n",
    ">单因子方差分析要解决的问题类似于独立样本$t$检验,它们都是想要得到不同组的均值之间的平均差异相对于各组内部平均差异而言是否统计显著,从而的出结论认为因为分组变量的取值不同,导致另外一个变量的均值产生了显著（或不显著）的差异.\n",
    "\n",
    ">但他们之间也存在着差别：\n",
    "* 支持比较的组数不同.独立样本$t$检验仅仅能够对两个独立样本进行检验,而单因子方差分析则可以对两个或更多的独立样本进行比较；\n",
    "* 所使用的比值不同.独立样本$t$检验用的是$t$值,而单因子方差分析使用的是$F$值（通过使用某种方差计算出来的值）.  \n",
    " -$t$值的分子是两个样本均值之间的简单差异（$\\bar{X_1}-\\bar{X_2}$）；而$F$值的分子是使用称为组间均方（$MS_b$）的量来计算三个及以上样本均值的平均差异的.  \n",
    " -$t$值的分母是均值之差的标准误,本质上是某种形式的标准差,而$F$值的分母是使用称为组内均方($MS_w$,也称均方误)的量来计算不同组的均值差异的.\n",
    "\n",
    ">要计算组间均方$MS_b$,需要先计算组间平方和$SS_b$\n",
    "$$SS_b=\\Sigma [n(\\bar{X}-\\bar{X_T})^2]$$\n",
    "然后用组间平方和除以$SS_b$的自由度($K-1$)就得到了组间均方\n",
    "$$MS_b=\\frac{SS_b}{K-1}$$\n",
    "要计算组内均方$MS_w$,需要先计算误差平方和$SS_e$\n",
    "$$SS_e=\\Sigma \\Sigma (X-\\bar{X_i})^2$$\n",
    "然后用组内均方除以$SS_e$的自由度（$N-K$）就得到了组内均方,也就是均方误\n",
    "$$MS_e=\\frac{SS_e}{N-K}$$\n",
    "上式中,$\\bar{X}$表示各组的均值,$\\bar{X_T}$表示所有组的样本合并之后计算出的均值,$n$表示各组样本的对象数.  \n",
    "最后我们给出$F$值的计算公式\n",
    "$$F=\\frac{MS_b}{MS_e}$$\n",
    "\n",
    ">从上边各式的意义可以看出来,$F$值实际上就是在比较,相对于均方误,也就是组内的平均差异（$MS_e$）而言,组间的平均差异($MS_b$)是否足够大？\n",
    "\n",
    ">当样本组数是两个的时候,$F$值近似等于$t$值的平方乘以一个与自由度有关的量.特别的,当使用单因子方差分析对两个独立样本进行比较时,所得结果是和独立样本$t$检验完全一致的.\n",
    "\n",
    ">使用单因子方差分析进行假设检验,和之前的假设检验过程是完全一致的.但是当分组变量的取值多于两个的时候,我们从分析结果只能得出结论：可能存在某两组使得这两组之间的均值差异是统计显著的,但是我们仍然不知道是哪两组之间存在显著的差异.因此我们还需要做\\textbf{事后检验},以期进一步确定到底是哪两组或哪几组之间存在着统计显著的差异.\n",
    "\n",
    ">事后检验的方法有很多,有些比较保守（判定组间差异统计显著的标准比较严格）,有些则比较宽松.所有事后检验的比较原则都是在控制比较的组数的条件下,对所有组的均值进行两两比较,然后确定其是否显著不同.常用的比较宽松的事后检验方法是$Tukey HSD$事后检验,此外还有相对严格的$Scheffe$事后检验.以下是进行$Tukey HSD$事后检验时需要计算的统计量\n",
    "$$Tukey HSD=\\frac{\\bar{X_1}-\\bar{X_2}}{s_{\\bar{X}}}$$\n",
    "式中的$s_{\\bar{X}}$表示某种标准误\n",
    "$$s_{\\bar{X}}=\\sqrt{\\frac{MS_e}{n_g}}$$\n",
    "其中$n_g$表示各组的对象数（各组对象数相等是使用$Tukey HSD$事后检验的必要的前提条件）.  \n",
    "在计算出了$Tukey HSD$值之后,还是要通过查表来确定差异是否是统计显著的.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 因子方差分析\n",
    ">因子方差分析进一步对单因子方差分析进行了推广.单因子方差分析只能考虑一个因素（即一个分组变量）,而因子方差分析则可以同时考虑多个影响因素（多个分组变量）,既能够检测每种影响因素的主效应,又能考虑不同因素之间的交互效应.\n",
    "\n",
    "## 复测方差分析\n",
    ">复测方差分析是对相依样本$t$检验的推广.就像单因子分析是对独立样本$t$检验在分组变量上从两个分组推广到多个分组类似,复测方差分析是把相依样本$t$检验在两个时间点的取值推广到了多个时间点.此外,还可以同时进行因子方差分析,将分组变量从一种推广到多种.我们甚至还可以考虑协变量的因素,进行协方差分析.限于时间关系,暂时不做展开.\n",
    "\n",
    ">以下给出复测方差分析的适用条件：\n",
    "* 正态性.各组数据服从正态分布.\n",
    "* 方差齐性.各组方差相等.\n",
    "* 球对称假设.对于自变量的各取值水平组合而言（对于被试内因素的各个水平组合而言）,因变量的协方差矩阵相等.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
